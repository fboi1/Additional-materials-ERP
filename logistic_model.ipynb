{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40cf4eb1",
   "metadata": {},
   "source": [
    "# Fit Logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77c96cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing AAPL_daily ===\n",
      "Logistic Regression Accuracy: 0.5363, AUC: 0.4990\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.10      0.16      1040\n",
      "           1       0.54      0.91      0.68      1205\n",
      "\n",
      "    accuracy                           0.54      2245\n",
      "   macro avg       0.52      0.51      0.42      2245\n",
      "weighted avg       0.52      0.54      0.44      2245\n",
      "\n",
      "Saved model to summary\\logistic\\AAPL_daily_logistic_model.pkl and scaler to summary\\logistic\\AAPL_daily_scaler.pkl\n",
      "\n",
      "=== Processing GE_daily ===\n",
      "Logistic Regression Accuracy: 0.4822, AUC: 0.4880\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.69      0.57      1582\n",
      "           1       0.48      0.28      0.35      1614\n",
      "\n",
      "    accuracy                           0.48      3196\n",
      "   macro avg       0.48      0.48      0.46      3196\n",
      "weighted avg       0.48      0.48      0.46      3196\n",
      "\n",
      "Saved model to summary\\logistic\\GE_daily_logistic_model.pkl and scaler to summary\\logistic\\GE_daily_scaler.pkl\n",
      "\n",
      "=== Processing IBM_daily ===\n",
      "Logistic Regression Accuracy: 0.5183, AUC: 0.5011\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.04      0.08      1524\n",
      "           1       0.52      0.95      0.67      1671\n",
      "\n",
      "    accuracy                           0.52      3195\n",
      "   macro avg       0.48      0.50      0.37      3195\n",
      "weighted avg       0.49      0.52      0.39      3195\n",
      "\n",
      "Saved model to summary\\logistic\\IBM_daily_logistic_model.pkl and scaler to summary\\logistic\\IBM_daily_scaler.pkl\n",
      "\n",
      "=== Processing JNJ_daily ===\n",
      "Logistic Regression Accuracy: 0.5113, AUC: 0.4920\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.18      0.26      1518\n",
      "           1       0.52      0.81      0.64      1676\n",
      "\n",
      "    accuracy                           0.51      3194\n",
      "   macro avg       0.49      0.50      0.45      3194\n",
      "weighted avg       0.49      0.51      0.46      3194\n",
      "\n",
      "Saved model to summary\\logistic\\JNJ_daily_logistic_model.pkl and scaler to summary\\logistic\\JNJ_daily_scaler.pkl\n",
      "\n",
      "=== Processing MSFT_daily ===\n",
      "Logistic Regression Accuracy: 0.5212, AUC: 0.5195\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.51      0.49       903\n",
      "           1       0.56      0.53      0.55      1077\n",
      "\n",
      "    accuracy                           0.52      1980\n",
      "   macro avg       0.52      0.52      0.52      1980\n",
      "weighted avg       0.52      0.52      0.52      1980\n",
      "\n",
      "Saved model to summary\\logistic\\MSFT_daily_logistic_model.pkl and scaler to summary\\logistic\\MSFT_daily_scaler.pkl\n",
      "\n",
      "Summary saved to summary\\stock_data_model_summary_logistic.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from glob import glob\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Paths\n",
    "input_folder = \"data/processed/stock_data\"\n",
    "summary_folder = \"summary\"\n",
    "model_folder = os.path.join(summary_folder, \"logistic\")\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "summary_results = []\n",
    "\n",
    "for file_path in glob(os.path.join(input_folder, \"*.csv\")):\n",
    "    stock_name = os.path.basename(file_path).replace(\"_features.csv\", \"\")\n",
    "    print(f\"\\n=== Processing {stock_name} ===\")\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Create binary target: 1 if price goes up tomorrow, 0 if down\n",
    "    df['Direction'] = (df['Close'].shift(-1) > df['Close']).astype(int)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Features\n",
    "    X = df.drop(columns=[\n",
    "        \"Date\",\"Close\",\"Target\",\"Direction\",\n",
    "        \"High\",\"Low\",\"Open\",\"High_lag1\",\"Low_lag1\",\"Open_lag1\"\n",
    "    ])\n",
    "    y = df['Direction']\n",
    "\n",
    "    # Split\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Logistic Regression\n",
    "    log_model = LogisticRegression(max_iter=1000)\n",
    "    log_model.fit(X_train_scaled, y_train)\n",
    "    y_pred = log_model.predict(X_test_scaled)\n",
    "    y_proba = log_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"Logistic Regression Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Save model and scaler\n",
    "    model_path = os.path.join(model_folder, f\"{stock_name}_logistic_model.pkl\")\n",
    "    scaler_path = os.path.join(model_folder, f\"{stock_name}_scaler.pkl\")\n",
    "    joblib.dump(log_model, model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"Saved model to {model_path} and scaler to {scaler_path}\")\n",
    "\n",
    "    # Append metrics to summary\n",
    "    summary_results.append({\n",
    "        \"Stock\": stock_name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"AUC\": auc\n",
    "    })\n",
    "\n",
    "# Save summary\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "os.makedirs(summary_folder, exist_ok=True)\n",
    "summary_file = os.path.join(summary_folder, \"stock_data_model_summary_logistic.csv\")\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "print(f\"\\nSummary saved to {summary_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f16957",
   "metadata": {},
   "source": [
    "# Test logistic model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class DonchianBreakoutBacktester:\n",
    "    def __init__(self, initial_capital=10000, commission=0.001):\n",
    "        self.initial_capital = initial_capital\n",
    "        self.commission = commission\n",
    "        \n",
    "    def load_model_and_scaler(self, model_path, scaler_path):\n",
    "        \"\"\"Load the trained model and scaler\"\"\"\n",
    "        model = joblib.load(model_path)\n",
    "        scaler = joblib.load(scaler_path)\n",
    "        return model, scaler\n",
    "    \n",
    "    def calculate_donchian_channels(self, df, period=20):\n",
    "        \"\"\"Calculate Donchian channel breakouts\"\"\"\n",
    "        df = df.copy()\n",
    "        df['DC_High'] = df['High'].rolling(window=period).max()\n",
    "        df['DC_Low'] = df['Low'].rolling(window=period).min()\n",
    "        df['DC_Mid'] = (df['DC_High'] + df['DC_Low']) / 2\n",
    "        return df\n",
    "    \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"Prepare features matching the training data format\"\"\"\n",
    "        # Remove columns that were excluded during training\n",
    "        feature_cols = [col for col in df.columns if col not in [\n",
    "            \"Date\", \"Close\", \"Target\", \"Direction\", \"High\", \"Low\", \"Open\", \n",
    "            \"High_lag1\", \"Low_lag1\", \"Open_lag1\", \"DC_High\", \"DC_Low\", \"DC_Mid\"\n",
    "        ]]\n",
    "        return df[feature_cols]\n",
    "    \n",
    "    def generate_signals(self, df, model, scaler, confidence_threshold=0.6, donchian_period=20):\n",
    "        \"\"\"Generate trading signals using ML model and Donchian breakouts\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Calculate Donchian channels\n",
    "        df = self.calculate_donchian_channels(df, period=donchian_period)\n",
    "        \n",
    "        # Prepare features for ML model\n",
    "        X = self.prepare_features(df)\n",
    "        \n",
    "        # Handle missing values\n",
    "        X = X.fillna(method='ffill').fillna(0)\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = scaler.transform(X)\n",
    "        \n",
    "        # Get ML predictions\n",
    "        ml_probabilities = model.predict_proba(X_scaled)[:, 1]\n",
    "        \n",
    "        # Generate combined signals\n",
    "        df['ML_Signal'] = (ml_probabilities > confidence_threshold).astype(int)\n",
    "        df['DC_Long_Signal'] = (df['Close'] > df['DC_High'].shift(1)).astype(int)\n",
    "        df['DC_Short_Signal'] = (df['Close'] < df['DC_Low'].shift(1)).astype(int)\n",
    "        \n",
    "        # Combined strategy: ML confirms Donchian breakouts\n",
    "        df['Long_Signal'] = (df['DC_Long_Signal'] & df['ML_Signal']).astype(int)\n",
    "        df['Short_Signal'] = (df['DC_Short_Signal'] & (1 - df['ML_Signal'])).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def backtest_strategy(self, df):\n",
    "        \"\"\"Execute the backtest with simple position logic\"\"\"\n",
    "        df = df.copy()\n",
    "        df['Position'] = 0\n",
    "        df['Portfolio_Value'] = self.initial_capital\n",
    "        df['Cash'] = self.initial_capital\n",
    "        df['Holdings'] = 0\n",
    "        df['Shares'] = 0\n",
    "        \n",
    "        position = 0  # 1 for long, -1 for short, 0 for neutral\n",
    "        cash = self.initial_capital\n",
    "        shares = 0\n",
    "        \n",
    "        for i in range(1, len(df)):\n",
    "            current_price = df.loc[df.index[i], 'Close']\n",
    "            prev_price = df.loc[df.index[i-1], 'Close']\n",
    "            \n",
    "            # Exit on opposite signal\n",
    "            if position != 0:\n",
    "                if (position == 1 and df.loc[df.index[i], 'Short_Signal']) or \\\n",
    "                   (position == -1 and df.loc[df.index[i], 'Long_Signal']):\n",
    "                    # Close position\n",
    "                    if position == 1:  # Close long\n",
    "                        cash = shares * current_price * (1 - self.commission)\n",
    "                        shares = 0\n",
    "                    else:  # Close short (simplified PnL)\n",
    "                        cash = cash + (shares * (prev_price - current_price)) * (1 - self.commission)\n",
    "                        shares = 0\n",
    "                    position = 0\n",
    "            \n",
    "            # Entry\n",
    "            if position == 0:\n",
    "                if df.loc[df.index[i], 'Long_Signal']:\n",
    "                    # Enter long\n",
    "                    shares = (cash * 0.95) / current_price  # Use 95% of cash\n",
    "                    cash = cash - (shares * current_price * (1 + self.commission))\n",
    "                    position = 1\n",
    "                elif df.loc[df.index[i], 'Short_Signal']:\n",
    "                    # Enter short (simplified)\n",
    "                    shares = (cash * 0.95) / current_price\n",
    "                    position = -1\n",
    "            \n",
    "            # Update portfolio value\n",
    "            if position == 1:\n",
    "                portfolio_value = cash + (shares * current_price)\n",
    "            elif position == -1:\n",
    "                portfolio_value = cash - (shares * (current_price - prev_price))\n",
    "            else:\n",
    "                portfolio_value = cash\n",
    "            \n",
    "            df.loc[df.index[i], 'Position'] = position\n",
    "            df.loc[df.index[i], 'Portfolio_Value'] = portfolio_value\n",
    "            df.loc[df.index[i], 'Cash'] = cash\n",
    "            df.loc[df.index[i], 'Shares'] = shares\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def calculate_returns(self, df):\n",
    "        \"\"\"Calculate returns and performance metrics\"\"\"\n",
    "        df = df.copy()\n",
    "        df['Strategy_Return'] = df['Portfolio_Value'].pct_change()\n",
    "        df['Benchmark_Return'] = df['Close'].pct_change()\n",
    "        df['Cumulative_Strategy'] = (1 + df['Strategy_Return']).cumprod()\n",
    "        df['Cumulative_Benchmark'] = (1 + df['Benchmark_Return']).cumprod()\n",
    "        return df\n",
    "    \n",
    "    def calculate_metrics(self, df):\n",
    "        \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
    "        strategy_returns = df['Strategy_Return'].dropna()\n",
    "        benchmark_returns = df['Benchmark_Return'].dropna()\n",
    "        \n",
    "        # Basic metrics\n",
    "        total_return = (df['Portfolio_Value'].iloc[-1] / self.initial_capital) - 1\n",
    "        benchmark_total_return = (df['Close'].iloc[-1] / df['Close'].iloc[0]) - 1\n",
    "        \n",
    "        # Volatility metrics\n",
    "        strategy_vol = strategy_returns.std() * np.sqrt(252)\n",
    "        benchmark_vol = benchmark_returns.std() * np.sqrt(252)\n",
    "        \n",
    "        # Risk-free rate (assumed)\n",
    "        risk_free_rate = 0.02\n",
    "        \n",
    "        # Sharpe ratio\n",
    "        excess_returns = strategy_returns - (risk_free_rate / 252)\n",
    "        sharpe_ratio = (excess_returns.mean() / strategy_returns.std() * np.sqrt(252)) if strategy_returns.std() > 0 else 0\n",
    "        \n",
    "        # Sortino ratio\n",
    "        downside_returns = strategy_returns[strategy_returns < 0]\n",
    "        downside_std = (downside_returns.std() * np.sqrt(252)) if len(downside_returns) > 0 else 0\n",
    "        sortino_ratio = (excess_returns.mean() / downside_std * np.sqrt(252)) if downside_std > 0 else 0\n",
    "        \n",
    "        # Maximum drawdown\n",
    "        cumulative = df['Cumulative_Strategy'].dropna()\n",
    "        running_max = cumulative.expanding().max()\n",
    "        drawdown = (cumulative - running_max) / running_max\n",
    "        max_drawdown = drawdown.min()\n",
    "        \n",
    "        # Win rate and profit factor (very simplified: uses returns around position flips)\n",
    "        trades = df[df['Position'] != df['Position'].shift()]['Strategy_Return'].dropna()\n",
    "        if len(trades) > 0:\n",
    "            win_rate = (trades > 0).sum() / len(trades)\n",
    "            winning_trades = trades[trades > 0].sum()\n",
    "            losing_trades = abs(trades[trades < 0].sum())\n",
    "            profit_factor = winning_trades / losing_trades if losing_trades > 0 else float('inf')\n",
    "        else:\n",
    "            win_rate = 0\n",
    "            profit_factor = 0\n",
    "        \n",
    "        return {\n",
    "            'Total_Return': total_return,\n",
    "            'Benchmark_Return': benchmark_total_return,\n",
    "            'Annual_Volatility': strategy_vol,\n",
    "            'Sharpe_Ratio': sharpe_ratio,\n",
    "            'Sortino_Ratio': sortino_ratio,\n",
    "            'Max_Drawdown': max_drawdown,\n",
    "            'Win_Rate': win_rate,\n",
    "            'Profit_Factor': profit_factor,\n",
    "            'Total_Trades': int(len(trades)) if len(trades) > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def monte_carlo_permutation_test(self, df, n_permutations=1000, random_state=42):\n",
    "        \"\"\"\n",
    "        Perform Monte Carlo permutation test on Sharpe ratio.\n",
    "        Returns dict including the full array of permuted Sharpes for plotting.\n",
    "        \"\"\"\n",
    "        strategy_returns = df['Strategy_Return'].dropna().values\n",
    "        \n",
    "        if len(strategy_returns) == 0:\n",
    "            return {\n",
    "                'p_value': 1.0, 'is_significant': False,\n",
    "                'actual_sharpe': 0.0,\n",
    "                'permuted_sharpes_mean': np.nan,\n",
    "                'permuted_sharpes_std': np.nan,\n",
    "                'permuted_sharpes': np.array([])\n",
    "            }\n",
    "        \n",
    "        # Actual strategy performance\n",
    "        actual_sharpe = self.calculate_sharpe_from_returns(strategy_returns)\n",
    "        \n",
    "        # Generate random permutations (reproducible)\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        permuted_sharpes = np.empty(n_permutations, dtype=float)\n",
    "        for i in range(n_permutations):\n",
    "            permuted_returns = rng.permutation(strategy_returns)\n",
    "            permuted_sharpes[i] = self.calculate_sharpe_from_returns(permuted_returns)\n",
    "        \n",
    "        # Calculate p-value (one-sided: permuted Sharpe >= actual Sharpe)\n",
    "        p_value = np.mean(permuted_sharpes >= actual_sharpe)\n",
    "        \n",
    "        return {\n",
    "            'p_value': float(p_value),\n",
    "            'is_significant': bool(p_value < 0.05),\n",
    "            'actual_sharpe': float(actual_sharpe),\n",
    "            'permuted_sharpes_mean': float(np.mean(permuted_sharpes)),\n",
    "            'permuted_sharpes_std': float(np.std(permuted_sharpes, ddof=1)),\n",
    "            'permuted_sharpes': permuted_sharpes\n",
    "        }\n",
    "    \n",
    "    def calculate_sharpe_from_returns(self, returns):\n",
    "        \"\"\"Helper function to calculate Sharpe ratio from returns\"\"\"\n",
    "        if len(returns) == 0 or np.std(returns) == 0:\n",
    "            return 0.0\n",
    "        excess_returns = returns - (0.02 / 252)  # Risk-free rate (daily)\n",
    "        return np.mean(excess_returns) / np.std(returns, ddof=1) * np.sqrt(252)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_permutation_distribution(stock_name, mc_results, save_dir):\n",
    "        \"\"\"\n",
    "        Plot histogram of permuted Sharpe ratios with vertical line at actual Sharpe.\n",
    "        \"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        perm = mc_results['permuted_sharpes']\n",
    "        if perm.size == 0:\n",
    "            print(f\"[{stock_name}] No returns to plot permutation distribution.\")\n",
    "            return None\n",
    "        \n",
    "        actual = mc_results['actual_sharpe']\n",
    "        pval = mc_results['p_value']\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(perm, bins=40, alpha=0.8, edgecolor='black')\n",
    "        plt.axvline(actual, linestyle='--', linewidth=2, label=f'Actual Sharpe = {actual:.3f}')\n",
    "        plt.title(f'{stock_name} â€” Monte Carlo Permutation Test (Sharpe)\\n'\n",
    "                  f'p-value = {pval:.4f}')\n",
    "        plt.xlabel('Permuted Sharpe Ratios')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        outpath = os.path.join(save_dir, f\"{stock_name}_mc_permutation_sharpe.png\")\n",
    "        plt.savefig(outpath, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"[{stock_name}] Permutation plot saved: {outpath}\")\n",
    "        return outpath\n",
    "\n",
    "\n",
    "def run_backtests():\n",
    "    \"\"\"Main function to run backtests for all stocks\"\"\"\n",
    "    # Paths\n",
    "    input_folder = \"data/processed/stock_data\"\n",
    "    model_folder = \"summary/logistic\"\n",
    "    output_folder = \"backtest_results_logistic\"\n",
    "    plots_folder = os.path.join(output_folder, \"permutation_plots_logistic\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    os.makedirs(plots_folder, exist_ok=True)\n",
    "    \n",
    "    backtester = DonchianBreakoutBacktester(initial_capital=10000)\n",
    "    results_summary = []\n",
    "    \n",
    "    # Get all available models\n",
    "    model_files = glob(os.path.join(model_folder, \"*_logistic_model.pkl\"))\n",
    "    \n",
    "    for model_file in model_files:\n",
    "        stock_name = os.path.basename(model_file).replace(\"_logistic_model.pkl\", \"\")\n",
    "        scaler_file = os.path.join(model_folder, f\"{stock_name}_scaler.pkl\")\n",
    "        data_file = os.path.join(input_folder, f\"{stock_name}_features.csv\")\n",
    "        \n",
    "        if not os.path.exists(scaler_file) or not os.path.exists(data_file):\n",
    "            print(f\"Missing files for {stock_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n=== Backtesting {stock_name} ===\")\n",
    "        \n",
    "        try:\n",
    "            # Load model and data\n",
    "            model, scaler = backtester.load_model_and_scaler(model_file, scaler_file)\n",
    "            df = pd.read_csv(data_file)\n",
    "            \n",
    "            # Use last 500 points for out-of-sample backtesting\n",
    "            df = df.tail(500).copy().reset_index(drop=True)\n",
    "            df.dropna(inplace=True)\n",
    "            \n",
    "            if len(df) < 50:  # Minimum data requirement\n",
    "                print(f\"Insufficient data for {stock_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Generate signals and run backtest\n",
    "            df = backtester.generate_signals(df, model, scaler, confidence_threshold=0.6, donchian_period=20)\n",
    "            df = backtester.backtest_strategy(df)\n",
    "            df = backtester.calculate_returns(df)\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            metrics = backtester.calculate_metrics(df)\n",
    "            \n",
    "            # Monte Carlo permutation test (return full distribution for plotting)\n",
    "            mc_results = backtester.monte_carlo_permutation_test(df, n_permutations=1000, random_state=42)\n",
    "            \n",
    "            # Plot and save permutation distribution for this stock\n",
    "            DonchianBreakoutBacktester.plot_permutation_distribution(\n",
    "                stock_name, mc_results, plots_folder\n",
    "            )\n",
    "            \n",
    "            # Combine results\n",
    "            result = {\n",
    "                'Stock': stock_name,\n",
    "                **metrics,\n",
    "                **{f'MC_{k}': v for k, v in mc_results.items() if k != 'permuted_sharpes'}  # exclude large array\n",
    "            }\n",
    "            results_summary.append(result)\n",
    "            \n",
    "            # Save individual backtest results\n",
    "            df.to_csv(os.path.join(output_folder, f\"{stock_name}_backtest.csv\"), index=False)\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"Total Return: {metrics['Total_Return']:.2%}\")\n",
    "            print(f\"Sharpe Ratio: {metrics['Sharpe_Ratio']:.3f}\")\n",
    "            print(f\"Sortino Ratio: {metrics['Sortino_Ratio']:.3f}\")\n",
    "            print(f\"Max Drawdown: {metrics['Max_Drawdown']:.2%}\")\n",
    "            print(f\"Profit Factor: {metrics['Profit_Factor']:.3f}\")\n",
    "            print(f\"Win Rate: {metrics['Win_Rate']:.2%}\")\n",
    "            print(f\"P-value: {mc_results['p_value']:.4f}\")\n",
    "            print(f\"Statistically Significant: {mc_results['is_significant']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {stock_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save summary results\n",
    "    if results_summary:\n",
    "        results_df = pd.DataFrame(results_summary)\n",
    "        results_df.to_csv(os.path.join(output_folder, \"backtest_summary.csv\"), index=False)\n",
    "        \n",
    "        # Overall summary\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"OVERALL BACKTEST SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Total Stocks Backtested: {len(results_df)}\")\n",
    "        print(f\"Average Return: {results_df['Total_Return'].mean():.2%}\")\n",
    "        print(f\"Average Sharpe: {results_df['Sharpe_Ratio'].mean():.3f}\")\n",
    "        print(f\"Average Sortino: {results_df['Sortino_Ratio'].mean():.3f}\")\n",
    "        print(f\"Average Profit Factor: {results_df['Profit_Factor'].mean():.3f}\")\n",
    "        print(f\"Profitable Strategies: {(results_df['Total_Return'] > 0).sum()}/{len(results_df)}\")\n",
    "        print(f\"Statistically Significant: {results_df['MC_is_significant'].sum()}/{len(results_df)}\")\n",
    "        \n",
    "        # Top performers\n",
    "        print(f\"\\nTop 5 by Total Return:\")\n",
    "        top_returns = results_df.nlargest(5, 'Total_Return')[['Stock', 'Total_Return', 'Sharpe_Ratio', 'MC_p_value']]\n",
    "        print(top_returns.to_string(index=False))\n",
    "        \n",
    "        print(f\"\\nTop 5 by Sharpe Ratio:\")\n",
    "        top_sharpe = results_df.nlargest(5, 'Sharpe_Ratio')[['Stock', 'Total_Return', 'Sharpe_Ratio', 'MC_p_value']]\n",
    "        print(top_sharpe.to_string(index=False))\n",
    "    \n",
    "    return results_summary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_backtests()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock-predict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
